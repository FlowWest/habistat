---
title: "Hydraulic Training Data"
output: github_document
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(stars)
library(terra)

# implement parallel processing
library(future) # parallel processing backend
library(future.apply) # parallelized versions of lapply, sapply, ...
library(furrr) # parallelized versions of purrr functions: map, map2, pmap,...
library(tictoc) # measure time to run
message(availableCores())
#if (availableCores()>1) plan(multisession) else plan(sequential)
plan(multisession, workers = 4)
# use plan(sequential) to run all as ordinary lapply, pmap, etc.
 
theme_set(theme_minimal())

#project_crs <- "EPSG:3310" # NAD83 California Teale Albers
#project_crs <- "EPSG:5070" # NAD83 CONUS Albers
project_crs <- "ESRI:102039" # NAD83 CONUS Albers USGS Version

library(googledrive)
drive_file_by_id <- function(id=character(), dir="temp", vsizip=FALSE) {
  d <- googledrive::as_dribble(googledrive::as_id(id))
  p <- file.path(dir, d$name)
  if(!file.exists(p)){
    googledrive::drive_download(file = d, path = p)
  }
  if(vsizip){
    return(file.path("/vsizip",p))
  } else {
    return(p)
  }
}
```

Define habitat suitability functions

Source: 1029_Yuba_Flow_Effects_Modeling_Task_1_2022-09-30_reduced.pdf

```{r funcs}
# simple linear interpolation function
linterp <- function(x, x1, x2, y1, y2){
  y1 + ((x-x1)/(x2-x1)) * (y2-y1)
}

# vectorized function to calculate the depth*velocity suitability factor
# if applicable, cover will also need to be applied.

# method=0 uses simple 1/0 thresholds (CVPIA -  see Deer Creek FR juvenile rearing floodplain criteria)
# method=1 uses ranges defined in CBEC Lower Yuba River report, tables 8-9
# method=2 uses ranges defined for USBR San Joaquin model (which also did not incorporate cover)
# method=3 uses simple 1/0 thresholds from HQT

dvhsi <- function(d, v, method=0) {
  
  if(method==0){
    
    return(if_else(d>0.5 & d<=5.2 & v>0 & v<=4.0, 1, 0))
    
  } else if(method==1){
    
    dhsi <- case_when(
      d<=0.5 ~ 0,
      d<=0.9 ~ linterp(d, 0.5, 0.9, 0, 1),
      d<=4.0 ~ 1,
      d<=5.2 ~ linterp(d, 4.0, 5.2, 1, 0),
      d>5.2 ~ 0)
    vhsi <- case_when(
      v<=0.1 ~ linterp(v, 0.0, 0.1, 0, 1),
      v<=0.8 ~ 1,
      v<=1.8 ~ linterp(v, 0.8, 1.8, 1, 0.35),
      v<=4.0 ~ linterp(v, 1.8, 4.0, 0.35, 0),
      v>4.0 ~ 0)
    return(sqrt(dhsi*vhsi))
    
  } else if(method==2){
    
    dhsi <- case_when(d<=0.01 ~ 0.00,
                      d<=0.10 ~ 0.10,
                      d<=0.20 ~ 0.20,
                      d<=0.60 ~ 0.15,
                      d<=0.80 ~ 0.35,
                      d<=1.00 ~ 0.46,
                      d<=1.20 ~ 0.53,
                      d<=1.50 ~ 0.64,
                      d<=2.10 ~ 0.86,
                      d<=2.50 ~ 0.86,
                      d<=2.90 ~ 0.57,
                      d<=4.00 ~ 0.34,
                      d<=4.50 ~ 0.14,
                      d<=5.00 ~ 0.30,
                      d<=7.00 ~ 0.20,
                      d<=7.10 ~ 0.10,
                      d<=10.0 ~ 0.00,
                      d>10.0 ~ 0.00)
    vhsi <- case_when(v<=0.1 ~ 0.94,
                      v<=0.2 ~ 0.98,
                      v<=0.3 ~ 1.00,
                      v<=0.5 ~ 0.99,
                      v<=0.7 ~ 0.98,
                      v<=1.0 ~ 0.96,
                      v<=1.2 ~ 0.95,
                      v<=1.5 ~ 0.93,
                      v<=2.0 ~ 0.68,
                      v<=2.5 ~ 0.30,
                      v<=2.8 ~ 0.08,
                      v<=3.4 ~ 0.01,
                      v<=3.5 ~ 0.01,
                      v>3.5 ~ 0.00)
    return(pmin(dhsi, vhsi))
    
  } else if(method==3){
    
    return(if_else(d>1 & d<=3.28 & v>0 & v<=1.5, 1, 0))

  }
}
```

## Stanislaus

Import SRH2D model data for Stanislaus (unlike HEC-RAS, the SRH2D outputs are natively vector format)

```{r stan-import, message=FALSE, warning=FALSE}
# SRH2D model domain, dissolved from SRH2D mesh faces using QGIS
stan_domain <- st_read("/vsizip/hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_Domain.shp.zip", as_tibble=T)

# SRH2D model domain, manually split into polygons aligning with COMID segments
stan_comid <- st_read("/vsizip/hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_Domain_COMID.shp.zip", as_tibble=T) |>
  st_zm() |> janitor::clean_names()

# SRH2D mesh vertices, converted from 2DM using QGIS 
stan_vertices <- st_read("/vsizip/hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_Vertices.shp.zip", as_tibble=T) |>
  mutate(vid = row_number()) |>
  select(vid)

# Thiessen (aka Voronoi) polygons for mesh vertices, generated using QGIS
stan_thiessen <- st_read("/vsizip/hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_Thiessen.shp.zip", as_tibble=T) |>
  #mutate(vid = row_number()) # row order doesn't match the SRH2D outputs so need to spatial join
  st_join(stan_vertices, join=st_nearest_feature) |>
  select(vid) |>
  arrange(vid)
# confirm correct join via:
# ggplot() + geom_sf(data=stan_thiessen|>filter(vid<50)) + geom_sf(data=stan_vertices|>filter(vid<50)) 

# Bed elevations, extracted from mesh using QGIS "Export time series values from points of a mesh dataset"
stan_elev <- read_csv("hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_BedElevation.csv.gz") |>
  janitor::clean_names() |>
  mutate(vid = row_number()) |>
  select(vid, bed_elevation)

# Material Manning coefficients, extracted from SRH2D DAT files for runs
stan_material_n <-
  read_delim("hydraulic_model_data/stanislaus_srh2d_2013/manning_coef.txt", delim="\t", col_names=c("manning_coef")) |>
  mutate(material_id = row_number())

# Material IDs (Manning's roughness classes) extracted from mesh using QGIS "Export time series values from points of a mesh dataset"
stan_material <- 
  read_csv("hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_MaterialID.csv.gz") |>
  janitor::clean_names() |>
  mutate(vid = row_number()) |>
  select(vid, material_id) |>
  left_join(stan_material_n)

# alternate approach with spatial join yields the same result
# stan_material <- 
#   read_csv("hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_MaterialID.csv.gz") |>
#   janitor::clean_names() |>
#   st_as_sf(coords=c("x","y"), crs=st_crs(stan_thiessen)) |>
#   st_join(stan_thiessen, join=st_nearest_feature)

# SRH2D results by vertex point, incl. depth, velocity, shear stress, froude
stan_result_filenames <- c(
   "500" = "hydraulic_model_data/stanislaus_srh2d_2013/500cfs_072313.csv.gz",
   "750" = "hydraulic_model_data/stanislaus_srh2d_2013/750cfs_072413.csv.gz",
  "1000" = "hydraulic_model_data/stanislaus_srh2d_2013/1000cfs_072413.csv.gz",
  "1250" = "hydraulic_model_data/stanislaus_srh2d_2013/1250cfs_090313.csv.gz",
  "1500" = "hydraulic_model_data/stanislaus_srh2d_2013/1500cfs_071013.csv.gz",
  "1750" = "hydraulic_model_data/stanislaus_srh2d_2013/1750cfs_090313.csv.gz",
  "2250" = "hydraulic_model_data/stanislaus_srh2d_2013/2250cfs_121713.csv.gz",
  "3000" = "hydraulic_model_data/stanislaus_srh2d_2013/3000cfs_061913.csv.gz",
  "5000" = "hydraulic_model_data/stanislaus_srh2d_2013/5000cfs_071113.csv.gz"
)
stan_result_cols <- c("x_m"="n", "y_m"="n", "z_m"="n", 
                      "wse_m"="n", "wdepth_m"="n", 
                      "vel_x"="n", "vel_y"="n", "vel_mag"="n", 
                      "froude"="n", "stress"="n")

stan_result <- 
  names(stan_result_filenames) |>
  lapply(function(x) read_csv(stan_result_filenames[x], col_names=names(stan_result_cols), col_types=stan_result_cols, skip=1) |> 
           mutate(discharge_cfs = as.numeric(x), vid=row_number())) |>
  bind_rows() |>
  mutate(across(everything(), function(x) if_else(x==-999,NA,x))) |>
  mutate(depth_ft = wdepth_m*3.28084, velocity_fps = vel_mag*3.28084) |>
  select(discharge_cfs, vid, depth_ft, velocity_fps) |>
  left_join(stan_elev, by=join_by(vid)) |> 
  left_join(stan_material, by=join_by(vid)) |>
  mutate(
    cover_hs = 1, # cover habitat suitability should be added here
    hsi_simp = dvhsi(depth_ft, velocity_fps, method=0) * cover_hs,
    hsi_frac = dvhsi(depth_ft, velocity_fps, method=1) * cover_hs,
    hsi_sjrm = dvhsi(depth_ft, velocity_fps, method=2) * cover_hs,
    hsi_shqt = dvhsi(depth_ft, velocity_fps, method=3) * cover_hs
  ) |>
  glimpse()

```

```{r fig.width=15, fig.height=10, dpi=300, eval=FALSE, include=FALSE}
# confirm that using row order is equivalent to spatial joining
read_csv("hydraulic_model_data/stanislaus_srh2d_2013/StanMesh072313_MaterialID.csv.gz") |>
  janitor::clean_names() |>
  mutate(vid = row_number()) |>
  st_as_sf(coords = c("x","y")) |>
  ggplot() + geom_sf(aes(color=factor(material_id)), size=1) + theme(legend.position = "top")
```

```{r fig.width=15, fig.height=10, dpi=300, eval=FALSE, include=FALSE}
stan_vertices |> left_join(stan_material) |> 
  ggplot() + geom_sf(aes(color=factor(material_id)), size=1) + theme(legend.position = "top")
```

Import polygons created for each COMID reach, and calculate suitable area for each

Proof of concept, first comid, 500 cfs:
```{r stan-test}
test <- stan_thiessen |> 
  left_join(filter(stan_result, discharge_cfs==500), by=join_by(vid)) |>
  st_intersection(stan_comid[1]) |>
    mutate(area_m2 = units::drop_units(st_area(geometry)),
           wua_simp = hsi_simp * area_m2,
           wua_frac = hsi_frac * area_m2,
           wua_sjrm = hsi_sjrm * area_m2,
           wua_shqt = hsi_shqt * area_m2) |>
  glimpse()

#test |> ggplot() + geom_sf(aes(fill=hsi_simp), color=NA)
```

Batch process, saving flow-to-suitable-area (fsa) by comid to file

```{r stan-calc-hsi}
stan_calc_hsi <- function(g, q){
  message(q)
  stan_thiessen |>
    left_join(filter(stan_result, discharge_cfs==q), by=join_by(vid)) |> 
    st_intersection(g) |>
    mutate(area_m2 = if_else(depth_ft>0, units::drop_units(st_area(geometry)), 0),
           wua_simp = hsi_simp * area_m2,
           wua_frac = hsi_frac * area_m2,
           wua_sjrm = hsi_sjrm * area_m2,
           wua_shqt = hsi_shqt * area_m2) |>
    summarize(area_m2 = sum(area_m2),
              wua_simp = sum(wua_simp),
              wua_frac = sum(wua_frac),
              wua_sjrm = sum(wua_sjrm),
              wua_shqt = sum(wua_shqt)) |>
    mutate(pcthab_simp = wua_simp / area_m2,
           pcthab_frac = wua_frac / area_m2,
           pcthab_sjrm = wua_sjrm / area_m2,
           pcthab_shqt = wua_shqt / area_m2) |>
    st_drop_geometry()
}

if(!file.exists("../data/fsa_stanislaus.Rds")) {
  tic("calculate stanislaus fsa")
  fsa_stanislaus <- 
    stan_comid |>
    expand_grid(flow_cfs = as.numeric(names(stan_result_filenames))) |>
    mutate(result = future_map2(geometry, flow_cfs, function(g, q) stan_calc_hsi(st_sfc(g, crs=st_crs(stan_comid)), q))) |>
    unnest_wider(result) |>
    st_as_sf() |>
    glimpse()
  toc()
  fsa_stanislaus |> saveRDS("../data/fsa_stanislaus.Rds")
} else {
  fsa_stanislaus <- readRDS("../data/fsa_stanislaus.Rds") |> glimpse()
}

#stan_hsi |> ggplot() + geom_sf(aes(fill=pcthab_simp), color=NA) + facet_wrap(~discharge_cfs)
```

```{r stan-plot-hsi, fig.width=6.5, fig.height=4, dpi=300}
fsa_stanislaus |> ggplot() + geom_line(aes(x = flow_cfs, y = pcthab_frac, color=factor(comid))) 
```

## Deer Creek

Import 2018 HEC-RAS 2D model outputs for Deer Creek

```{r deer-import-lc}
# get land cover data
dir.create("temp/deer_land_cover", recursive = TRUE)
drive_file_by_id("1OMJzuVoFdt1hygfMVPWrQBGr2BIuxSZj") |>
  archive::archive_extract(dir="temp/deer_land_cover")
deer_lcc <- terra::rast("temp/deer_land_cover/existing_landcover_20181109.tif")
terra::cats(deer_lcc)[[1]] |> glimpse()
ggplot() + tidyterra::geom_spatraster(data=deer_lcc)

# merge in other attributes
deer_lcc_table <- 
  read_csv("hydraulic_model_data/deercreek_hecras2d_2018/existing_landcover_classes.csv") |>
  janitor::clean_names() |>
  select(id, cover_type, cover_hsi) 
deer_lcc <- deer_lcc |> 
  terra::addCats(value=deer_lcc_table, merge=TRUE, layer=1)
terra::activeCat(deer_lcc, layer=1) <- "cover_hsi"
ggplot() + tidyterra::geom_spatraster(data=deer_lcc)
```

```{r deer-import-flow}
dir.create("temp/deer_model_output", recursive = TRUE)
# "temp/deer_model_output/Existing_Habitat.tar.gz"
drive_file_by_id("1rmMw6PXJGS0-ui52eaotABCSlJsZOzvr", dir="temp/deer_model_output") |>
  archive::archive_extract(dir="temp/deer_model_output")

filenames <- tribble(~flow_cfs, ~timestep,
        100, "15NOV2018 06 00 00",
        250, "15NOV2018 16 00 00",
        300, "16NOV2018 02 00 00",
        400, "16NOV2018 12 00 00",
        500, "16NOV2018 22 00 00",
        600, "17NOV2018 08 00 00",
       1000, "17NOV2018 18 00 00",
       3000, "18NOV2018 04 00 00",
       5000, "18NOV2018 14 00 00",
       6000, "19NOV2018 00 00 00",
       7000, "19NOV2018 10 00 00",
       9000, "19NOV2018 20 00 00",
      10000, "20NOV2018 06 00 00",
      11000, "20NOV2018 16 00 00",
      12000, "21NOV2018 02 00 00",
      13000, "21NOV2018 12 00 00",
      14000, "21NOV2018 22 00 00",
      15000, "22NOV2018 08 00 00") |>
  mutate(depth = paste0("temp/deer_model_output/Depth (",timestep,").vrt"),
         velocity = paste0("temp/deer_model_output/Velocity (",timestep,").vrt"))

dep_r <- terra::rast(filenames$depth)
terra::set.names(dep_r, filenames$flow_cfs)
dep_r_packed <- terra::wrap(dep_r)

vel_r <- terra::rast(filenames$velocity)
terra::set.names(vel_r, filenames$flow_cfs)
vel_r_packed <- terra::wrap(vel_r)

deer_comids <- 
  st_read("/vsizip/hydraulic_model_data/deercreek_hecras2d_2018/model_perim_split_comid_generalized.shp.zip", as_tibble=T) |>
  janitor::clean_names() |> filter(comid>0)

raster_hsi <- function(g, q, crs="EPSG:6416"){
  comid <- terra::vect(st_sfc(g, crs=crs))
  dep_r_unpacked <- terra::unpack(dep_r_packed)
  vel_r_unpacked <- terra::unpack(vel_r_packed)  
  d <- dep_r_unpacked[[as.character(q)]] |> terra::crop(comid) 
  v <- vel_r_unpacked[[as.character(q)]] |> terra::crop(comid) 
  ext <- (d>0)
  # LYR version
  hsi <- (d>0.5 & d<=5.2) & (v>0 & v<=4.0)
  area_wua <- terra::global(hsi, "sum", na.rm=TRUE)[[1]]
  area_tot <- terra::global(ext, "sum", na.rm=TRUE)[[1]]
  area_pct <- area_wua / area_tot
  # HQT version
  hsi_hqt <- (d>1 & d<=3.28) & (v>0 & v<=1.5)
  area_wua_hqt <- terra::global(hsi_hqt, "sum", na.rm=TRUE)[[1]]
  area_tot_hqt <- terra::global(ext, "sum", na.rm=TRUE)[[1]]
  area_pct_hqt <- area_wua_hqt / area_tot_hqt
  return(list("area_tot"=area_tot, 
              "area_wua"=area_wua, 
              "area_pct"=area_pct,
              "area_tot_hqt"=area_tot_hqt, 
              "area_wua_hqt"=area_wua_hqt, 
              "area_pct_hqt"=area_pct_hqt))
}

# getting a memory error doing this all at once:
# deer_hsi_result <- 
#   deer_comids |> 
#   expand_grid(flow_cfs=filenames$flow_cfs) |>
#   mutate(result = pmap(list(geometry, flow_cfs), raster_hsi)) |>
#   unnest_wider(result)

run_for_flow <- function(q){
  message(paste0("calculating HSI for q=",q))
  deer_comids |> 
    mutate(result = map(geometry, function(g) raster_hsi(g, q))) |>
    unnest_wider(result) |> 
    st_drop_geometry() |>
    mutate(flow_cfs=q)
}

if(!file.exists("../data/fsa_deercreek.Rds")) {
  tic("calculate deer fsa")
  deer_hsi_result <- bind_rows(future_lapply(filenames$flow_cfs, run_for_flow))
  toc()
  deer_hsi_result |> saveRDS("../data/fsa_deercreek.Rds")
} else {
  deer_hsi_result <- readRDS("../data/fsa_deercreek.Rds")
}

deer_hsi_result |> glimpse()
```

```{r deer-plot-hsi, fig.width=6.5, fig.height=4, dpi=300}
# examples
deer_hsi_result |> ggplot() + geom_line(aes(x = flow_cfs, y = area_pct, color=factor(comid)))
```

## Yuba River

Import SRH2D model data for Yuba (adapted version of Stanislaus methods)

```{r yuba-import, message=FALSE, warning=FALSE}

# IMPORT DATA

flows <- c(300, 350, 400, 450, 530, 600, 622, 700, 800, 880, 930,
           1000, 1300, 1500, 1700, 2000, 2500, 3000, 4000, 5000, 7500, 
           10000, 15000, 21100, 30000, 42200, 84400, 110400)

# extract CSV results
dir.create("temp", recursive = TRUE)
yuba_csv_results <- drive_file_by_id("155QA16y1PP5wFAc21Uvwb_gj2tNqdYDG") 

if(!file.exists("temp/lyr_srh2d_csv.tar")){
  yuba_csv_results |> archive::archive_extract(dir="temp", files="lyr_srh2d_csv.tar")
  yuba_csv_results |> archive::archive_extract(dir="temp")
}

yuba_gdrive_ids <- tribble(
  ~reach, ~domain, ~vertices, ~thiessen,
  "EDR", "17KmDTcYfTDJTF-6WcbhQd_5WUbqrVs3k", "1t_oT37auM_25SnU3TLrXa_xCyoV9H9qt", "1s-FhTTDlclwcuu-H-9wIwwC1K7lEijXI", 
  "TBR", "1IArTq6Yh_R44csnUYEayNtbRFYr3xPsl", "1tHAvrhIaw_Gz47K5pL0SpeniEMYnczln", "1E7ucN5WOyynzNjeYTiPiS_ixbaJXA6cW",
  "HR" , "1HXYUeop_otp0H06x69NPrE02SNradmEC", "1cByc4uVCf46lwVnvfECmKi3IOsR3T2sU", "1ca5tpyAAe2DzeiVStl4Cl8qEvwI7Jaxx",
  "DGR", "17ybIFHUBlKmMuQkXVIrQOOPPnoDWQkRe", "1aSsCAHVwRyquDH_-tA89-7qObJ4eqc-v", "1aCvZAd7YqU5kzQfshmxIBNECWQqRJKoC",
  "FR" , "1f0N27qss7FsXHfml8U_B-FtoVApbaVKV", "1teWo-kp-ujtKtHfvoLfZZCKcGsh0twiD", "1lkGlWPqBDHHP8yN8U-lkxKBnkRKmfVgS")

# FUNCTIONS FOR IMPORTING ALL REACHES, ALL FLOWS

r <- "EDR" # FOR TESTING PURPOSES
q <- 700  # FOR TESTING PURPOSES

result_cols <- c("vid"="n", 
                 "x_ft"="n", "y_ft"="n", "z_ft"="n", 
                 "wse_ft"="n", "wdepth_ft"="n", 
                 "vel_x"="n", "vel_y"="n", "vel_mag"="n", 
                 "froude"="n", "stress"="n")

get_reach_flow_result <- function(r, q) {
  message(q)
  # returns a data frame of results for a particular reach and flow
  #yuba_csv_results |>
  #  archive::archive_read(paste0("temp/", r, "_", q, "_SMS.csv"))
  filename <- paste0("temp/", r, "_", q, "_SMS.csv") 
  if(file.exists(filename)) {
    out <- filename |>
    read_csv(col_names=names(result_cols), col_types=result_cols, skip=1) |>
    mutate(discharge_cfs = q, 
           vid=row_number())
    return(out)
  } else {
    return(NULL)
  }
}

get_reach_result <- function(r) {
  message(r)
  # returns a merged data frame of results for all flows of a particular reach
  flows |> 
    future_lapply(function(q) get_reach_flow_result(r, q)) |> 
    bind_rows() |>
    transmute(discharge_cfs, reach=r, vid, depth_ft=wdepth_ft, velocity_fps=vel_mag) 
}

# COMBINE RESULT

if(!file.exists("temp/hsi_yuba.Rds")) {

  tic("calculate yuba hsi")
yuba_result <- 
  yuba_gdrive_ids$reach |>
  lapply(get_reach_result) |> # don't use future_lapply here because get_reach_result is already parallelized
  bind_rows() |>
  mutate(across(everything(), function(x) if_else(x==-999,NA,x))) |>
  mutate(
    cover_hs = 1, # cover habitat suitability should be added here
    hsi_simp = dvhsi(depth_ft, velocity_fps, method=0) * cover_hs,
    hsi_frac = dvhsi(depth_ft, velocity_fps, method=1) * cover_hs,
    hsi_sjrm = dvhsi(depth_ft, velocity_fps, method=2) * cover_hs,
    hsi_shqt = dvhsi(depth_ft, velocity_fps, method=3) * cover_hs) |>
  glimpse()
  toc()

  yuba_result |> saveRDS("temp/hsi_yuba.Rds")
  
} else {
  
  yuba_result <- readRDS("temp/hsi_yuba.Rds") |> glimpse()
  
}

```

```{r yuba-calc-hsi}

# IMPORT COMID REACH POLYGON DELINEATIONS

# allow sufficient sized files to be passed through parallel nodes
message(object.size(yuba_result))
options(future.globals.maxSize = 8000 * 1024^2)

yuba_comid <- drive_file_by_id("1-xSi142jtNZKQS9-VgXZT1yzTfH1KjyR", vsizip=T) |>
  st_read(as_tibble=T) |> st_zm() |> janitor::clean_names()

yuba_calc_comid_hsi <- function(g, q, t){
  message(q)
  t |>
    left_join(filter(yuba_result, discharge_cfs==q), by=join_by(vid)) |> 
    st_intersection(g) |>
    mutate(area_ft2 = if_else(depth_ft>0, units::drop_units(st_area(geometry)), 0),
           wua_simp = hsi_simp * area_ft2,
           wua_frac = hsi_frac * area_ft2,
           wua_sjrm = hsi_sjrm * area_ft2,
           wua_shqt = hsi_shqt * area_ft2) |>
    summarize(area_ft2 = sum(area_ft2),
              wua_simp = sum(wua_simp),
              wua_frac = sum(wua_frac),
              wua_sjrm = sum(wua_sjrm),
              wua_shqt = sum(wua_shqt)) |>
    mutate(pcthab_simp = wua_simp / area_ft2,
           pcthab_frac = wua_frac / area_ft2,
           pcthab_sjrm = wua_sjrm / area_ft2,
           pcthab_shqt = wua_shqt / area_ft2) |>
    st_drop_geometry()
}

r_q_combos <- yuba_result |> rename(flow_cfs = discharge_cfs) |> group_by(reach, flow_cfs) |> summarize() |> ungroup()

yuba_calc_reach_hsis <- function(r) {
  message(r)
  ids <- yuba_gdrive_ids |> filter(reach==r)
  
  # vertices
  v <- drive_file_by_id(ids[["vertices"]], vsizip=T) |>
    st_read(as_tibble=T) |> 
    mutate(vid=row_number()) |> 
    select(vid)
  
  # thiessen
  t <- drive_file_by_id(ids[["thiessen"]], vsizip=T) |>
    st_read(as_tibble=T) |> 
    st_join(v, join=st_nearest_feature) |>
    select(vid) |>
    arrange(vid)
  
  yuba_comid |>
    filter(reach_lyr==r) |>
    #expand_grid(flow_cfs = flows) |>
    inner_join(r_q_combos, by=join_by(reach_lyr==reach), relationship="many-to-many") |>
    mutate(result = future_map2(geometry, flow_cfs, function(g, q) yuba_calc_comid_hsi(st_sfc(g, crs=st_crs(yuba_comid)), q, t))) |>
    unnest_wider(result) |>
    st_as_sf() |>
    glimpse()

}

if(!file.exists("../data/fsa_yuba.Rds")) {
  
  tic("calculate yuba fsa")
  fsa_yuba <-
    yuba_gdrive_ids$reach |>
    lapply(yuba_calc_reach_hsis) |> # don't use future_lapply here because yuba_calc_reach_hsis is already parallelized
    bind_rows()
  toc()
    
  fsa_yuba |> saveRDS("../data/fsa_yuba.Rds")
  
} else {
  
  fsa_yuba <- readRDS("../data/fsa_yuba.Rds") |> glimpse()
  
}


```

```{r yuba-plot-hsi, fig.width=6.5, fig.height=4, dpi=300}
fsa_yuba |> ggplot() + geom_line(aes(x = flow_cfs, y = pcthab_frac, color=factor(comid))) 
```

## Combined flow-to-suitable-area training set

Combining the results of the above two imports into a consistent dataset. `fsa_combined.Rds` should be joined by `comid` with the flowlines and the predictor variables to use for modeling. 

For now, just bringing over the simple threshold-based HSI calculation that uses depth and velocity thresholds (`(d>0.5 & d<=5.2) & (v>0.1 & v<=4.0)`). To be improved. 

```{r}
fsa_combined <- 
  bind_rows(
    deer_hsi_result |> 
      st_drop_geometry() |>
      transmute(dataset = "Deer Creek",
                comid, 
                flow_cfs, 
                area_tot_ft2 = area_tot, 
                area_wua_ft2 = area_wua,
                hsi_frac = area_pct),
    fsa_stanislaus |> 
      st_drop_geometry() |>
      transmute(dataset = "Stanislaus River",
                comid,
                flow_cfs,
                area_tot_ft2 = area_m2 / 3.281^2,
                area_wua_ft2 = wua_simp / 3.281^2,
                hsi_frac = pcthab_simp),
    fsa_yuba |> 
      st_drop_geometry() |>
      transmute(dataset = "Lower Yuba River",
                comid,
                flow_cfs,
                area_tot_ft2 = area_ft2,
                area_wua_ft2 = wua_simp,
                hsi_frac = pcthab_simp)
          ) |> arrange(dataset, comid, flow_cfs)

fsa_combined |> glimpse() |> saveRDS("../data/fsa_combined.Rds")
```
